{
 "metadata": {
  "name": "Web scraping with Python"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Web scraping with Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is an introduction to web scraping using Python, where our task is to extract information from web pages.\n",
      "\n",
      "Prerequisites (knowledge):\n",
      "\n",
      " * basic Python (its data structures, string manipulation)\n",
      " * basic HTML\n",
      " * basic HTTP (know what a GET request is: this will be reviewed)\n",
      " * bonus: knowledge of how to use XPath\n",
      "\n",
      "Prerequisites (software):\n",
      "\n",
      " * the `lxml` package\n",
      "\n",
      "Rather than using Scrapy or another Python web scraping framework, we'll go the barebones route."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "HTTP basics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "HTTP is a protocol for transferring data across the internet. Every communication looks like this: a **client** (such as a browser) sends a request to a **web server**, and the server sends a response back to the client.\n",
      "\n",
      "There's a few types of **requests** that the client can send, such as the `GET` and `POST` request types.\n",
      "\n",
      "* The `GET` request is the most common type. Semantically, it is used for requesting data from the web server. \n",
      "* `POST` requests are usually used when we're sending data to the server that's large or changes some kind of state on the server, such as if it causes a database to be updated.\n",
      "\n",
      "The request and **response** each follow a simple text-based format: the first line is specific to requests and responses, then several lines of headers are specified in a `Header-Name: value` format, then a blank line follows the headers and precedes the body. The body contains the main payload, and a header tells the client/server how large the body is.\n",
      "\n",
      "An example request (from Wikipedia):\n",
      "\n",
      "    GET /index.html HTTP/1.1\n",
      "    Host: www.example.com\n",
      "\n",
      "And a corresponding response, showing us a status code (everyone's seen 404 Not Found) among other things:\n",
      "\n",
      "    HTTP/1.1 200 OK\n",
      "    Date: Mon, 23 May 2005 22:38:34 GMT\n",
      "    Server: Apache/1.3.3.7 (Unix) (Red-Hat/Linux)\n",
      "    Last-Modified: Wed, 08 Jan 2003 23:11:55 GMT\n",
      "    Etag: \"3f80f-1b6-3e1cb03b\"\n",
      "    Content-Type: text/html; charset=UTF-8\n",
      "    Content-Length: 131\n",
      "    Connection: close\n",
      "    \n",
      "    <html>\n",
      "    <head>\n",
      "      <title>An Example Page</title>\n",
      "    </head>\n",
      "    <body>\n",
      "      Hello World, this is a very simple HTML document.\n",
      "    </body>\n",
      "    </html>\n",
      "\n",
      "The body doesn't necessarily have to be plain text as in this example: it could be a sequence of non-text bytes whose length is specified by `Content-Length`.\n",
      "\n",
      "Let's try simulating that same request."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "response = urllib2.urlopen(\"http://example.com\")\n",
      "print response.read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<!doctype html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Example Domain</title>\n",
        "\n",
        "    <meta charset=\"utf-8\" />\n",
        "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
        "    <style type=\"text/css\">\n",
        "    body {\n",
        "        background-color: #f0f0f2;\n",
        "        margin: 0;\n",
        "        padding: 0;\n",
        "        font-family: \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
        "        \n",
        "    }\n",
        "    div {\n",
        "        width: 600px;\n",
        "        margin: 5em auto;\n",
        "        padding: 50px;\n",
        "        background-color: #fff;\n",
        "        border-radius: 1em;\n",
        "    }\n",
        "    a:link, a:visited {\n",
        "        color: #38488f;\n",
        "        text-decoration: none;\n",
        "    }\n",
        "    @media (max-width: 700px) {\n",
        "        body {\n",
        "            background-color: #fff;\n",
        "        }\n",
        "        div {\n",
        "            width: auto;\n",
        "            margin: 0 auto;\n",
        "            border-radius: 0;\n",
        "            padding: 1em;\n",
        "        }\n",
        "    }\n",
        "    </style>    \n",
        "</head>\n",
        "\n",
        "<body>\n",
        "<div>\n",
        "    <h1>Example Domain</h1>\n",
        "    <p>This domain is established to be used for illustrative examples in documents. You may use this\n",
        "    domain in examples without prior coordination or asking for permission.</p>\n",
        "    <p><a href=\"http://www.iana.org/domains/example\">More information...</a></p>\n",
        "</div>\n",
        "</body>\n",
        "</html>\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Python's `urlopen` function in the `urllib2` module returns a file-like object, so we can call `read()` to read all its contents.\n",
      "\n",
      "We can access the response's headers, too, using the `.info()` method of the response object. It returns a `mimetools.Message` instance that we can use like a `dict`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print response.info()\n",
      "print \"The content type is '%s'.\" % response.info()['content-type']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accept-Ranges: bytes\r\n",
        "Content-Type: text/html; charset=UTF-8\r\n",
        "Date: Tue, 02 Jul 2013 05:54:23 GMT\r\n",
        "ETag: \"780602-4f6-4db31b2978ec0\"\r\n",
        "Last-Modified: Thu, 25 Apr 2013 16:13:23 GMT\r\n",
        "Server: ECS (sea/1C15)\r\n",
        "X-Cache: HIT\r\n",
        "Content-Length: 1270\r\n",
        "Connection: close\r\n",
        "\n",
        "The content type is 'text/html; charset=UTF-8'.\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This was a simple `GET` request. We can also send `POST` requests using the `data` keyword argument of the `urlopen()` function."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Massaging out information"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For those familiar with regular expressions (affectionately referred to as regex's), regexes look like something we could use here to extract information from HTML. That's partially correct.\n",
      "\n",
      "Let's try extracting the page title of a website."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "source = urllib2.urlopen(\"http://ncix.com\").read()\n",
      "print re.search(r'<title>(.*?)</title>', source).group(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "NCIX.com - Canada's Premier Computer Store - Online PC Discount Store, Buy Computer Accessories\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can handle simple stuff with regexes, but HTML tags are simply too complicated for all but the simplest of cases.\n",
      "A tag with attributes can span multiple lines, there can be arbitrary whitespace in a tag, etc.\n",
      "However, regexes will still prove useful to process text that's inside an HTML page, and might be useful for extracting text from some Javascript source in a page.\n",
      "\n",
      "What can we do instead? HTML, like XML, has a recursive containment structure, so lends itself well to a recursive (nested) data structure with classes representing each tag. There's parsers for HTML source that nicely handle constructing these representations of HTML. For Python, we've got\n",
      "\n",
      "* BeautifulSoup\n",
      "* html5lib\n",
      "* lxml (a wrapper for the C++ libxml library)\n",
      "\n",
      "We'll stick with the last one, but the other two are good too.\n",
      "\n",
      "Let's try using it to read all the `<a>` tags (hyperlinks) from the NCIX homepage."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import lxml.html\n",
      "html = lxml.html.fromstring(source)\n",
      "print html.xpath('//a')[:10]  # just print out the first 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[<Element a at 0x1bd7170>, <Element a at 0x1bd7050>, <Element a at 0x1bd7110>, <Element a at 0x1bd70b0>, <Element a at 0x1bd71d0>, <Element a at 0x1bd7230>, <Element a at 0x1bd7290>, <Element a at 0x1bd72f0>, <Element a at 0x1bd7350>, <Element a at 0x1bd73b0>]\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, we've got a bunch of `<a>` tags. The `xpath()` method performs an **XPath query**. XPath is a query language for XML that searches the hierarchical structure that XML or HTML has (called the DOM or Document Object Model).\n",
      "\n",
      "Our query was `//a`. An XPath query consists of slash-delimited parts. Here, the double slash `//` means \"any number of parents\", followed by an `<a>` tag.\n",
      "\n",
      "Using an HTML inspector like the one built into Google Chrome (press F12 to activate it on a page), we can determine what the structure of a certain node in the DOM is. For example, if we open up the NCIX page and inspect the \"Popular Categories\" on the side, we find that each category link is inside a certain `div` tag:\n",
      "\n",
      "    <div id=\"sublinks\"> ...\n",
      "\n",
      "and each link looks like:\n",
      "\n",
      "    <a href=\"http://www.ncix.com/products/?minorcatid=1263\" class=\"sub_link\">Blu-Ray Drives<span class=\"qtycount\"> (6)</span></a>\n",
      "\n",
      "Let's grab all of these links using an Xpath query. In XPath, an at-sign `@` before an identifier means \"attribute\":"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "html.xpath('//div[@id=\"sublinks\"]/a[@class=\"sub_link\"]/@href')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "['http://ncix.com/products/?minorcatid=1263',\n",
        " 'http://ncix.com/products/?minorcatid=1265',\n",
        " 'http://ncix.com/products/?minorcatid=1084',\n",
        " 'http://ncix.com/products/?minorcatid=1015',\n",
        " 'http://ncix.com/products/?minorcatid=104',\n",
        " 'http://ncix.com/products/?minorcatid=1228',\n",
        " 'http://ncix.com/products/?minorcatid=1303',\n",
        " 'http://ncix.com/products/?minorcatid=1020',\n",
        " 'http://ncix.com/products/?minorcatid=1272',\n",
        " 'http://ncix.com/products/?minorcatid=109',\n",
        " 'http://ncix.com/products/?minorcatid=1031',\n",
        " 'http://ncix.com/products/?minorcatid=1051',\n",
        " 'http://ncix.com/products/?minorcatid=101',\n",
        " 'http://ncix.com/products/?minorcatid=1032',\n",
        " 'http://ncix.com/products/?minorcatid=1003',\n",
        " 'http://ncix.com/products/?minorcatid=1331',\n",
        " 'http://ncix.com/products/?minorcatid=102',\n",
        " 'http://ncix.com/products/?minorcatid=1216',\n",
        " 'http://ncix.com/products/?minorcatid=107',\n",
        " 'http://ncix.com/products/?minorcatid=1045',\n",
        " 'http://ncix.com/products/?minorcatid=1004',\n",
        " 'http://ncix.com/products/?minorcatid=1191',\n",
        " 'http://ncix.com/products/?minorcatid=1005',\n",
        " 'http://ncix.com/products/?minorcatid=1055',\n",
        " 'http://ncix.com/products/?minorcatid=1066',\n",
        " 'http://ncix.com/products/?minorcatid=106',\n",
        " 'http://ncix.com/products/?minorcatid=1012',\n",
        " 'http://ncix.com/products/?minorcatid=1019',\n",
        " 'http://ncix.com/products/?minorcatid=1275',\n",
        " 'http://ncix.com/products/?minorcatid=103',\n",
        " 'http://ncix.com/products/?minorcatid=1036',\n",
        " 'http://ncix.com/products/?minorcatid=108']"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here strings got returned directly because we requested the `href` attributes. But instead, we could've gotten a bunch of `Element` instances and manipulated these. This is useful when you want to do some more complex manipulation, or if you want to extensively query the children of a specific element."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}